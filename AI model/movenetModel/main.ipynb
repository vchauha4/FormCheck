{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import signal\n",
    "\n",
    "# TFLite interpreter\n",
    "num_kps = 17\n",
    "input_size = 256\n",
    "interpreter = tf.lite.Interpreter(model_path=\"lite-model_movenet_singlepose_thunder_tflite_int8_4.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "COLORS= {\n",
    "    'm': (62, 74, 179),\n",
    "    'c': (3, 4, 5),\n",
    "    'y': (92, 13, 30),\n",
    "}\n",
    "\n",
    "EDGE_TO_COLOR = {\n",
    "    (0, 1): COLORS['m'],\n",
    "    (0, 2): COLORS['c'],\n",
    "    (1, 3): COLORS['m'],\n",
    "    (2, 4): COLORS['c'],\n",
    "    (0, 5): COLORS['m'],\n",
    "    (0, 6): COLORS['c'],\n",
    "    (5, 7): COLORS['m'],\n",
    "    (7, 9): COLORS['m'],\n",
    "    (6, 8): COLORS['c'],\n",
    "    (8, 10): COLORS['c'],\n",
    "    (5, 6): COLORS['y'],\n",
    "    (5, 11): COLORS['m'],\n",
    "    (6, 12): COLORS['c'],\n",
    "    (11, 12): COLORS['y'],\n",
    "    (11, 13): COLORS['m'],\n",
    "    (13, 15): COLORS['m'],\n",
    "    (12, 14): COLORS['c'],\n",
    "    (14, 16): COLORS['c'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(p1, p2, p3):\n",
    "    radians = np.arctan2(p3[1] - p2[1], p3[0] - p2[0]) - np.arctan2(p1[1] - p2[1], p1[0] - p2[0])\n",
    "    joint_angle = np.abs(radians * 180.0 / np.pi)\n",
    "\n",
    "    if joint_angle > 180.0:\n",
    "        joint_angle = 360 - joint_angle\n",
    "\n",
    "    return joint_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes if the given joints are horizontal according to the y axis position, within a given error.\n",
    "def horizontal_joints(joints_y, y_axis, error):\n",
    "    for y in joints_y:\n",
    "        if not(y_axis - error <= y <= y_axis + error):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Computes if the given joints are vertical according to the x axis position, within a given error.\n",
    "def vertical_joints(joints_x, x_axis, error):\n",
    "    for x in joints_x:\n",
    "        if not(x_axis - error <= x <= x_axis + error):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def joint_in_region(x, y, x_axis, y_axis, x_error, y_error):\n",
    "    return x_axis - x_error <= x <= x_axis + x_error and y_axis - y_error <= y <= y_axis + y_error\n",
    "\n",
    "\n",
    "def angle_in_region(angle, expected_radius, radius_error):\n",
    "    return expected_radius - radius_error <= angle <= expected_radius + radius_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draws the angle on the reference joint.\n",
    "def visualize_angle(frame, pos, angle):\n",
    "    cv2.putText(\n",
    "        img=frame,\n",
    "        text=str(angle),\n",
    "        org=pos,\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=2.0,\n",
    "        color=(0, 255, 0),\n",
    "        thickness=2,\n",
    "        lineType=cv2.LINE_AA\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Computes the pose similarity of 2 different poses, using the Mean Absolute Error Method.\n",
    "# def compute_pose_similarity(self, joints1, joints2):\n",
    "#     if len(joints2) == len(self._exercise_joints):\n",
    "#         mae = 0\n",
    "#     #     for j in self._exercise_joints:# NEED TO CHANGE THIS\n",
    "#     #         mae += abs(np.linalg.norm(np.array(joints1[j]) - np.array(joints2[j])))\n",
    "#     #     return mae\n",
    "#     # else:\n",
    "#     #     return 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    # TF Lite format expects tensor type of uint8.\n",
    "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    interpreter.invoke() # Invoke inference.\n",
    "\n",
    "    # Get the model prediction.\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    print(\"PoseNetData\")\n",
    "\n",
    "    print(keypoints_with_scores[0][0][5])\n",
    "    print(keypoints_with_scores[0][0][7])\n",
    "    print(keypoints_with_scores[0][0][9])\n",
    "\n",
    "    print(calculate_angle(keypoints_with_scores[0][0][5], keypoints_with_scores[0][0][7], keypoints_with_scores[0][0][9]))\n",
    "    \n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(image, width, height):\n",
    "    image_width = image.shape[1]\n",
    "    image_height = image.shape[0]\n",
    "\n",
    "    # get resize ratio\n",
    "    resize_ratio = min(width / image_width, height / image_height)\n",
    "\n",
    "    # compute new height and width\n",
    "    new_width = int(resize_ratio * image_width)\n",
    "    new_height = int(resize_ratio * image_height)\n",
    "    new_img = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "    # compute padded height and width\n",
    "    pad_width = (width - new_width) // 2\n",
    "    pad_height = (height - new_height) // 2\n",
    "\n",
    "    padded_image = cv2.copyMakeBorder(new_img,\n",
    "                                      pad_height,\n",
    "                                      pad_height,\n",
    "                                      pad_width,\n",
    "                                      pad_width,\n",
    "                                      cv2.BORDER_REPLICATE,\n",
    "                                      value=0)\n",
    "\n",
    "    return cv2.resize(padded_image, (input_size, input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference(image):\n",
    "    # Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = pad(image, input_size, input_size)\n",
    "    image = cv2.resize(image, (input_size, input_size))\n",
    "    input_image = image\n",
    "\n",
    "    input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "    # Run model inference.\n",
    "    kps = movenet(input_image)[0]\n",
    "    \n",
    "\n",
    "    return kps[0], image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_kps(kps, height, width):\n",
    "    for i in range(len(kps)):\n",
    "        temp = kps[i][1]\n",
    "        kps[i][1] = kps[i][0] * height\n",
    "        kps[i][0] = temp * width\n",
    "    return kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pose(image, keypoints, radius=2):\n",
    "    height, width, channel = image.shape\n",
    "    kps = preprocess_kps(keypoints, height, width)\n",
    "    for c in kps:\n",
    "        x, y, s = c\n",
    "        if s > 0.2:\n",
    "            cv2.circle(image,\n",
    "                       (int(x), int(y)),\n",
    "                       radius, (41, 128, 185), -1)\n",
    "    for edge_pair, color in EDGE_TO_COLOR.items():\n",
    "        start, end = edge_pair\n",
    "        x1, y1, s1 = kps[start]\n",
    "        x2, y2, s2 = kps[end]\n",
    "        cv2.line(image,\n",
    "                 (int(x1), int(y1)),\n",
    "                 (int(x2), int(y2)),\n",
    "                 color, 1,\n",
    "                 lineType=cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need one that calcs the Initial pose for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(vidPath):\n",
    "    cap = cv2.VideoCapture(vidPath)\n",
    "    fname = 'op_' + str(vidPath.split(\"/\")[-1])\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # 25\n",
    "\n",
    "    ## Writing the video with keypoints\n",
    "    size = (input_size * 2, input_size * 2)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    video_writer = cv2.VideoWriter(fname, fourcc, fps, size)\n",
    "\n",
    " \n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        curr_kp, image = get_inference(frame)\n",
    "\n",
    "\n",
    "        output = draw_pose(image, curr_kp)\n",
    "        output = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
    "        outimage = np.asarray(output, dtype=np.uint8)\n",
    "        outimage = cv2.resize(outimage, size)\n",
    "\n",
    "        video_writer.write(outimage)\n",
    "        cv2.imshow(\"frame\", outimage)\n",
    "\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == ord('q') or k == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoseNetData\n",
      "[0.5017696  0.6021235  0.88311446]\n",
      "[0.50979793 0.6703642  0.92727023]\n",
      "[0.43352893 0.75466144 0.88311446]\n",
      "131.1525447642418\n",
      "PoseNetData\n",
      "[0.5017696  0.6101518  0.81888795]\n",
      "[0.51381207 0.6703642  0.92727023]\n",
      "[0.42148647 0.7506473  0.81888795]\n",
      "119.69916446958752\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ret:\n\u001b[0;32m     17\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m curr_kp, image \u001b[39m=\u001b[39m get_inference(frame)\n\u001b[0;32m     22\u001b[0m output \u001b[39m=\u001b[39m draw_pose(image, curr_kp)\n\u001b[0;32m     23\u001b[0m output \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(output, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n",
      "Cell \u001b[1;32mIn [11], line 11\u001b[0m, in \u001b[0;36mget_inference\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      8\u001b[0m input_image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(input_image, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Run model inference.\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m kps \u001b[39m=\u001b[39m movenet(input_image)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[39mreturn\u001b[39;00m kps[\u001b[39m0\u001b[39m], image\n",
      "Cell \u001b[1;32mIn [9], line 17\u001b[0m, in \u001b[0;36mmovenet\u001b[1;34m(input_image)\u001b[0m\n\u001b[0;32m     14\u001b[0m output_details \u001b[39m=\u001b[39m interpreter\u001b[39m.\u001b[39mget_output_details()\n\u001b[0;32m     16\u001b[0m interpreter\u001b[39m.\u001b[39mset_tensor(input_details[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m], input_image\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m---> 17\u001b[0m interpreter\u001b[39m.\u001b[39;49minvoke() \u001b[39m# Invoke inference.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# Get the model prediction.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m keypoints_with_scores \u001b[39m=\u001b[39m interpreter\u001b[39m.\u001b[39mget_tensor(output_details[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\TheRealJoker\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:917\u001b[0m, in \u001b[0;36mInterpreter.invoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[39m\"\"\"Invoke the interpreter.\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \n\u001b[0;32m    907\u001b[0m \u001b[39mBe sure to set the input sizes, allocate tensors and fill values before\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[39m  ValueError: When the underlying interpreter fails raise ValueError.\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_safe()\n\u001b[1;32m--> 917\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpreter\u001b[39m.\u001b[39;49mInvoke()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vidPath='Y2Mate.is - NFL Combine Bench Press Compilation!!!!!-vIsQ15POK7Q-720p-1655143445064.mp4'\n",
    "cap = cv2.VideoCapture(vidPath)\n",
    "fname = 'op_' + str(vidPath.split(\"/\")[-1])\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # 25\n",
    "\n",
    "## Writing the video with keypoints\n",
    "size = (input_size * 2, input_size * 2)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "video_writer = cv2.VideoWriter(fname, fourcc, fps, size)\n",
    "\n",
    " \n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    curr_kp, image = get_inference(frame)\n",
    "\n",
    "\n",
    "    output = draw_pose(image, curr_kp)\n",
    "    output = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
    "    outimage = np.asarray(output, dtype=np.uint8)\n",
    "    outimage = cv2.resize(outimage, size)\n",
    "\n",
    "    video_writer.write(outimage)\n",
    "    cv2.imshow(\"frame\", outimage)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == ord('q') or k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3506eeefc577687cfd871861fe500c69b7116de4ff9db4c4b451db1b4761ef90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
